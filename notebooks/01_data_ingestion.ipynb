{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Cleaning\n",
    "\n",
    "This notebook loads raw CSV datasets, cleans column names, normalizes city names, parses dates, and handles missing values.\n",
    "\n",
    "## Data Sources\n",
    "- `weather_india.csv`: Weather data (rainfall, temperature)\n",
    "- `traffic_india.csv`: Traffic congestion levels\n",
    "- `demand_india.csv`: Demand index for quick-commerce orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path for config imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set up paths\n",
    "RAW_DATA_DIR = Path('../data/raw')\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all raw CSV files\n",
    "weather_df = pd.read_csv(RAW_DATA_DIR / 'weather_india.csv')\n",
    "traffic_df = pd.read_csv(RAW_DATA_DIR / 'traffic_india.csv')\n",
    "demand_df = pd.read_csv(RAW_DATA_DIR / 'demand_india.csv')\n",
    "\n",
    "print(\"Weather data shape:\", weather_df.shape)\n",
    "print(\"Traffic data shape:\", traffic_df.shape)\n",
    "print(\"Demand data shape:\", demand_df.shape)\n",
    "\n",
    "print(\"\\nWeather columns:\", weather_df.columns.tolist())\n",
    "print(\"Traffic columns:\", traffic_df.columns.tolist())\n",
    "print(\"Demand columns:\", demand_df.columns.tolist())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Column Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names: strip whitespace, lowercase, replace spaces with underscores\n",
    "def clean_column_names(df):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    return df\n",
    "\n",
    "weather_df = clean_column_names(weather_df)\n",
    "traffic_df = clean_column_names(traffic_df)\n",
    "demand_df = clean_column_names(demand_df)\n",
    "\n",
    "print(\"Cleaned column names:\")\n",
    "print(\"Weather:\", weather_df.columns.tolist())\n",
    "print(\"Traffic:\", traffic_df.columns.tolist())\n",
    "print(\"Demand:\", demand_df.columns.tolist())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize City Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize city names: title case, strip whitespace\n",
    "def normalize_city_names(df, city_col='city'):\n",
    "    df = df.copy()\n",
    "    df[city_col] = df[city_col].str.strip().str.title()\n",
    "    return df\n",
    "\n",
    "weather_df = normalize_city_names(weather_df)\n",
    "traffic_df = normalize_city_names(traffic_df)\n",
    "demand_df = normalize_city_names(demand_df)\n",
    "\n",
    "print(\"Unique cities in each dataset:\")\n",
    "print(\"Weather:\", sorted(weather_df['city'].unique()))\n",
    "print(\"Traffic:\", sorted(traffic_df['city'].unique()))\n",
    "print(\"Demand:\", sorted(demand_df['city'].unique()))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date columns\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "traffic_df['date'] = pd.to_datetime(traffic_df['date'])\n",
    "demand_df['date'] = pd.to_datetime(demand_df['date'])\n",
    "\n",
    "print(\"Date ranges:\")\n",
    "print(f\"Weather: {weather_df['date'].min()} to {weather_df['date'].max()}\")\n",
    "print(f\"Traffic: {traffic_df['date'].min()} to {traffic_df['date'].max()}\")\n",
    "print(f\"Demand: {demand_df['date'].min()} to {demand_df['date'].max()}\")\n",
    "\n",
    "print(\"\\nDate column dtypes:\")\n",
    "print(f\"Weather date dtype: {weather_df['date'].dtype}\")\n",
    "print(f\"Traffic date dtype: {traffic_df['date'].dtype}\")\n",
    "print(f\"Demand date dtype: {demand_df['date'].dtype}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(\"\\nWeather:\")\n",
    "print(weather_df.isnull().sum())\n",
    "print(\"\\nTraffic:\")\n",
    "print(traffic_df.isnull().sum())\n",
    "print(\"\\nDemand:\")\n",
    "print(demand_df.isnull().sum())\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For weather: forward fill rainfall, interpolate temperature\n",
    "# For traffic: forward fill congestion\n",
    "# For demand: forward fill demand_index\n",
    "\n",
    "def handle_missing_values(df, dataset_name):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by date and city to ensure proper forward fill\n",
    "    df = df.sort_values(['city', 'date'])\n",
    "    \n",
    "    if dataset_name == 'weather':\n",
    "        # Forward fill rainfall (assume same as previous day if missing)\n",
    "        df['rainfall_mm'] = df.groupby('city')['rainfall_mm'].ffill().fillna(0)\n",
    "        # Interpolate temperature (linear interpolation)\n",
    "        df['temperature'] = df.groupby('city')['temperature'].interpolate(method='linear')\n",
    "        # If still missing, forward fill\n",
    "        df['temperature'] = df.groupby('city')['temperature'].ffill()\n",
    "        \n",
    "    elif dataset_name == 'traffic':\n",
    "        # Forward fill congestion level\n",
    "        df['congestion_level'] = df.groupby('city')['congestion_level'].ffill()\n",
    "        # If still missing, use median by city\n",
    "        city_medians = df.groupby('city')['congestion_level'].transform('median')\n",
    "        df['congestion_level'] = df['congestion_level'].fillna(city_medians)\n",
    "        \n",
    "    elif dataset_name == 'demand':\n",
    "        # Forward fill demand index\n",
    "        df['demand_index'] = df.groupby('city')['demand_index'].ffill()\n",
    "        # If still missing, use median by city\n",
    "        city_medians = df.groupby('city')['demand_index'].transform('median')\n",
    "        df['demand_index'] = df['demand_index'].fillna(city_medians)\n",
    "    \n",
    "    return df\n",
    "\n",
    "weather_df = handle_missing_values(weather_df, 'weather')\n",
    "traffic_df = handle_missing_values(traffic_df, 'traffic')\n",
    "demand_df = handle_missing_values(demand_df, 'demand')\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(\"\\nWeather:\")\n",
    "print(weather_df.isnull().sum())\n",
    "print(\"\\nTraffic:\")\n",
    "print(traffic_df.isnull().sum())\n",
    "print(\"\\nDemand:\")\n",
    "print(demand_df.isnull().sum())\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Data Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data ranges and types\n",
    "print(\"Weather data validation:\")\n",
    "print(f\"Rainfall range: {weather_df['rainfall_mm'].min():.2f} - {weather_df['rainfall_mm'].max():.2f} mm\")\n",
    "print(f\"Temperature range: {weather_df['temperature'].min():.2f} - {weather_df['temperature'].max():.2f} \u00b0C\")\n",
    "\n",
    "print(\"\\nTraffic data validation:\")\n",
    "print(f\"Congestion range: {traffic_df['congestion_level'].min():.2f} - {traffic_df['congestion_level'].max():.2f}\")\n",
    "\n",
    "print(\"\\nDemand data validation:\")\n",
    "print(f\"Demand index range: {demand_df['demand_index'].min():.2f} - {demand_df['demand_index'].max():.2f}\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicate check (date, city combinations):\")\n",
    "print(f\"Weather duplicates: {weather_df.duplicated(subset=['date', 'city']).sum()}\")\n",
    "print(f\"Traffic duplicates: {traffic_df.duplicated(subset=['date', 'city']).sum()}\")\n",
    "print(f\"Demand duplicates: {demand_df.duplicated(subset=['date', 'city']).sum()}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned datasets\n",
    "weather_df.to_csv(PROCESSED_DATA_DIR / 'weather_cleaned.csv', index=False)\n",
    "traffic_df.to_csv(PROCESSED_DATA_DIR / 'traffic_cleaned.csv', index=False)\n",
    "demand_df.to_csv(PROCESSED_DATA_DIR / 'demand_cleaned.csv', index=False)\n",
    "\n",
    "print(\"Cleaned data saved to:\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'weather_cleaned.csv'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'traffic_cleaned.csv'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'demand_cleaned.csv'}\")\n",
    "\n",
    "print(\"\\nData ingestion complete!\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}