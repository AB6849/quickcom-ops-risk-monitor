{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This notebook creates rolling 7-day features for operational risk monitoring and merges all datasets into a single daily city-level feature table.\n",
    "\n",
    "## Features Created\n",
    "- Rolling 7-day averages for congestion, rainfall, and demand by city\n",
    "- Merged daily city-level feature table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path for config imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Set up paths\n",
    "PROCESSED_DATA_DIR = Path('../data/processed')\n",
    "\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned datasets\n",
    "weather_df = pd.read_csv(PROCESSED_DATA_DIR / 'weather_cleaned.csv', parse_dates=['date'])\n",
    "traffic_df = pd.read_csv(PROCESSED_DATA_DIR / 'traffic_cleaned.csv', parse_dates=['date'])\n",
    "demand_df = pd.read_csv(PROCESSED_DATA_DIR / 'demand_cleaned.csv', parse_dates=['date'])\n",
    "\n",
    "print(\"Loaded cleaned datasets:\")\n",
    "print(f\"Weather: {weather_df.shape}\")\n",
    "print(f\"Traffic: {traffic_df.shape}\")\n",
    "print(f\"Demand: {demand_df.shape}\")\n",
    "\n",
    "# Ensure data is sorted by city and date\n",
    "weather_df = weather_df.sort_values(['city', 'date']).reset_index(drop=True)\n",
    "traffic_df = traffic_df.sort_values(['city', 'date']).reset_index(drop=True)\n",
    "demand_df = demand_df.sort_values(['city', 'date']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Rolling 7-Day Features\n",
    "\n",
    "For near-real-time operational risk monitoring, we compute rolling 7-day averages to smooth out daily fluctuations and capture trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, value_col, window=7, group_col='city'):\n",
    "    \"\"\"\n",
    "    Create rolling window features for a given column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with date and group_col columns\n",
    "        value_col: Column name to compute rolling average for\n",
    "        window: Rolling window size (default 7 days)\n",
    "        group_col: Column to group by (default 'city')\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added rolling features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.sort_values([group_col, 'date']).reset_index(drop=True)\n",
    "    \n",
    "    # Create rolling 7-day average\n",
    "    rolling_col = f'{value_col}_7d_avg'\n",
    "    df[rolling_col] = df.groupby(group_col)[value_col].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Create rolling 7-day max (for peak detection)\n",
    "    rolling_max_col = f'{value_col}_7d_max'\n",
    "    df[rolling_max_col] = df.groupby(group_col)[value_col].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create rolling features for each dataset\n",
    "print(\"Creating rolling features...\")\n",
    "\n",
    "# Traffic: rolling congestion\n",
    "traffic_df = create_rolling_features(traffic_df, 'congestion_level', window=7)\n",
    "print(\"[OK] Traffic rolling features created\")\n",
    "\n",
    "# Weather: rolling rainfall\n",
    "weather_df = create_rolling_features(weather_df, 'rainfall_mm', window=7)\n",
    "print(\"[OK] Weather rolling features created\")\n",
    "\n",
    "# Demand: rolling demand index\n",
    "demand_df = create_rolling_features(demand_df, 'demand_index', window=7)\n",
    "print(\"[OK] Demand rolling features created\")\n",
    "\n",
    "print(\"\\nSample rolling features:\")\n",
    "print(traffic_df[['date', 'city', 'congestion_level', 'congestion_level_7d_avg']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets\n",
    "\n",
    "Merge all datasets into a single daily city-level feature table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets on date and city\n",
    "# Start with weather as base (has both rainfall and temperature)\n",
    "daily_features = weather_df[['date', 'city', 'rainfall_mm', 'rainfall_mm_7d_avg', \n",
    "                              'rainfall_mm_7d_max', 'temperature']].copy()\n",
    "\n",
    "# Merge traffic data\n",
    "daily_features = daily_features.merge(\n",
    "    traffic_df[['date', 'city', 'congestion_level', 'congestion_level_7d_avg', \n",
    "                'congestion_level_7d_max']],\n",
    "    on=['date', 'city'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Merge demand data\n",
    "daily_features = daily_features.merge(\n",
    "    demand_df[['date', 'city', 'demand_index', 'demand_index_7d_avg', \n",
    "               'demand_index_7d_max']],\n",
    "    on=['date', 'city'],\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Sort by date and city\n",
    "daily_features = daily_features.sort_values(['date', 'city']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Merged feature table shape: {daily_features.shape}\")\n",
    "print(f\"\\nDate range: {daily_features['date'].min()} to {daily_features['date'].max()}\")\n",
    "print(f\"Unique cities: {sorted(daily_features['city'].unique())}\")\n",
    "print(f\"\\nColumns: {daily_features.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values in Merged Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after merge\n",
    "print(\"Missing values in merged dataset:\")\n",
    "print(daily_features.isnull().sum())\n",
    "\n",
    "# Forward fill missing values by city (for cases where one dataset has more dates)\n",
    "daily_features = daily_features.sort_values(['city', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Forward fill within each city group\n",
    "for col in daily_features.columns:\n",
    "    if col not in ['date', 'city']:\n",
    "        daily_features[col] = daily_features.groupby('city')[col].ffill()\n",
    "        # If still missing, backward fill\n",
    "        daily_features[col] = daily_features.groupby('city')[col].bfill()\n",
    "        # If still missing, fill with 0 for numeric columns\n",
    "        if daily_features[col].dtype in ['float64', 'int64']:\n",
    "            daily_features[col] = daily_features[col].fillna(0)\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(daily_features.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Feature Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature table\n",
    "print(\"Feature table summary:\")\n",
    "print(daily_features.describe())\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(daily_features.head(10))\n",
    "\n",
    "print(\"\\nData completeness by city:\")\n",
    "city_completeness = daily_features.groupby('city').agg({\n",
    "    'date': 'count',\n",
    "    'rainfall_mm': lambda x: x.notna().sum(),\n",
    "    'congestion_level': lambda x: x.notna().sum(),\n",
    "    'demand_index': lambda x: x.notna().sum()\n",
    "})\n",
    "print(city_completeness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save daily city features\n",
    "output_path = PROCESSED_DATA_DIR / 'daily_city_features.csv'\n",
    "daily_features.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Daily city features saved to: {output_path}\")\n",
    "print(f\"Total records: {len(daily_features)}\")\n",
    "print(f\"Date range: {daily_features['date'].min().date()} to {daily_features['date'].max().date()}\")\n",
    "print(f\"Unique cities: {daily_features['city'].nunique()}\")\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
